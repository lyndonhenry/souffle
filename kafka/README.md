# README

Welcome to Souffle on Kafka.

## Experiments

The experiments are managed by four bash scripts.

The scripts do not depend on each other, and no other script depends on them -- they are merely helper scripts.

The scripts are:

1. `script-to-run-in-docker.sh` 
2. `script-to-generate-datasets.sh` 
3. `script-to-generate-experiments.sh`
4. `script-to-run-experiments.sh`

Each script is to be run from the Souffle root directory, i.e. `./kafka/my-script.sh` and not `cd kafka && ./my-script.sh`.

The scripts correspond to each of the steps involved in running the experiments, and are presented in order.

The purpose of each script is as follows:

- `script-to-run-in-docker.sh`
    - Pulls and runs the Docker image for Souffle on Kafka.
    - The script shows which parameters to pass `docker run`, to mount directories.
    - All other scripts should be run from within the Docker container that this script pulls and runs.
    
- `script-to-generate-datasets.sh`
    - Downloads all datasets for the experiments, formats them, and places them in the S3 bucket.

- `script-to-generate-experiments.sh`
    - Generates all `docker-compose.yml` files for the experiments.
    - Each Docker compose file is placed in a directory that uniquely identifies the experiment it runs.
    - These are placed a subdirectory of `s3://souffle-on-kafka/docker-compose/`.
    - This subdirectory is either `example`, `yes-cloud`, or `no-cloud`.
    - The `example` docker compose files are used to test the system.
    - The `yes-cloud` docker compose files **are to be run in Azure**.
    - The `no-cloud` docker compose files **are to be run on plang8**.
    - Note that the user will be prompted to continue generating the real experiments once the example experiments are generated.
    - By terminating the script before the real experiments are generated, the system can be tested on the examples only.
    
- `script-to-run-experiments.sh`
    - Runs the experiments, using Docker compose commands.
    - A user's AWS credentials are written to a `.env` file; the script then changes to the directory of the `.env` file, so that calls to `docker-compose` use the contents of this file to populate the environment variables.
    - Each experiment generates a log file in `s3://souffle-on-kafka/output/log`; we check for the existence of this log file in the S3 bucket to signal an experiment's termination.
    

## Results

Results are obtained from a log file generated by an experiment.

The log file is generated to the S3 bucket at the conclusion of an experiment.

The path of the log file is `s3://souffle-on-kafka/output/log/<experiment-name>.log`, where `<experiment-name>` is the unique name of the experiment. 

In particular, the experiment name will be the same as the directory containing the `docker-compose.yml` file that defines the experiment, in the S3 bucket at `s3://souffle-on-kafka/docker-compose/`.

The schema of log messages is described by the following grammar.

~~~
LOG_MESSAGE := HEAD,BODY
HEAD := <stratum-name>,<message-index>,<timestamp>
BODY := beginBashScript
    | endBashScript
    | beginSouffleProgram
    | endSouffleProgram
    | downloadInput,<line-count>
    | uploadOutput,<line-count>
    | printMetadata,<benchmark-program>,<data-type>,<split-size>,<join-type>,<kafka-mode>,<evaluation-algorithm>,<dataset-name>,<thread-count>,<unique-id>
    | beginClient
    | endClient
    | beginProduction,<topic-name>
    | endProduction,<topic-name>
    | beginConsumption,<topic-name>
    | endConsumption,<topic-name>
    | beginProduce,<topic-name>,<payload-size>,<payload-type-size>
    | endProduce,<topic-name>
    | beginConsume,<topic-name>
    | endConsume,<topic-name>,<payload-size>,<payload-type-size>
    | beginPollProducer
    | endPollProducer
    | beginPollConsumer
    | endPollConsumer
~~~

A log message is composed of a `HEAD` and a `BODY`.

The `HEAD` of a log message consists of each of the following fields.

- `<stratum-name>` is the name of the stratum emitting this message, or "main". Note that the -2 stratum is responsible for reading and producing input files, while the -3 stratum does the same for output files.
- `<message-index>` is the index or count of the current message.
- `<timestamp>` is the timestamp in milliseconds since the last message.

The `BODY` of a log message contains information about the operation which cased the message to be emitted.

The `BODY` of a log message starts with the name of the operation to which the message corresponds, followed by a number of values involved in that operation.

The following message bodies are emitted by operations from the bash script that is responsible for managing execution of the Souffle program.

- `printMetadata,<benchmark-program>,<data-type>,<split-size>,<join-type>,<kafka-mode>,<evaluation-algorithm>,<dataset-name>,<thread-count>,<unique-id>`
    - Emmitted immediately after the bash script managing the Souffle program execution begins. This effectively gives the input parameters of the experiment.
    - `<benchmark-program>` is the name of the benchmark program in use.
    - `<data-type>` is the Souffle data type, either `number` or `symbol`.
    - `<split-size>` is the number of splits in the Souffle program for the IDB relation.
    - `<join-type>` is the type of join used for the splits in the Souffle program.
    - `<kafka-mode>` is the Kafka mode, either `no-kafka` for no use of Kafka, `one-kafka` for running each Souffle program strata as a separate process in the same Docker container, or `many-kafka` for running each Souffle program strata in a separate Docker container.
    - `<evaluation-algorithm>` is the evaluation algorithm used, either `SNE`, `GSNE`, `GPSNE`, or `GPCSNE`.
    - `<dataset-name>` is the name of the dataset used in the experiment.
    - `<thread-count>` is the number of threads used by the Souffle program.
    - `<unique-id>` is the unique identifier of this experiment.
- `beginBashScript`
    - Emmitted immediately after the `printMetadata` message is emitted.
- `endBashScript`
    - Emitted immediately before the bash script managing the Souffle program execution ends.
- `beginSouffleProgram`
    - Emitted immediately before execution of the Souffle program.
- `endSouffleProgram`
    - Emitted immediately after the execution of the Souffle program.
- `downloadInput,<line-count>`
    - Emitted immediately after the input for the Souffle program has been downloaded.
    - `<line-count>` is the number of lines in the downloaded file (or files).
- `uploadOutput,<line-count>`
    - Emitted immediately after the output for the Souffle program has been uploaded.
    - `<line-count>` is the number of lines in the uploaded file (or files).

The following message bodies are mmitted by operations from within the execution of the Souffle program itself.

- `beginClient`
    - Emitted immediately after the Souffle program's client is connected to Kafka.
- `endClient`
    - Emitted immediately before the Souffle program's client terminates its connection, along with the program itself.
- `beginProduction,<topic-name>`
    - Emitted immediately after instantiation of a producer for a given topic.
    - `<topic-name>` is the name of the topic on which to produce.
- `endProduction,<topic-name>`
    - Emmitted immediately after terminating all production on a given topic.
    - `<topic-name>` is the name of the topic.
- `beginConsumption,<topic-name>`
    - Emitted immediately after instantiation of a consumer for a given topic.
    - `<topic-name>` is the name of the topic on which to consume.
- `endConsumption,<topic-name>`
    - Emmitted immediately after terminating all consumption on a given topic.
    - `<topic-name>` is the name of the topic.
- `beginProduce,<topic-name>,<payload-size>,<payload-type-size>`
    - Emitted before production of a message on given topic.
    - `<topic-name>` The name of the topic.
    - `<payload-size>` Number of elements in the payload vector.
    - `<payload-type-size>` Size in bytes of each element in the payload vector.
- `endProduce,<topic-name>`
    - Emitted after production of a message on a given topic. 
    - `<topic-name>` is the name of the topic.
- `beginConsume,<topic-name>`
    - Emitted before consumption of a message on a given topic. 
    - `<topic-name>` is the name of the topic.
- `endConsume,<topic-name>,<payload-size>,<payload-type-size>`
    - Emitted after consumption of a message on given topic.
    - `<topic-name>` is the name of the topic.
    - `<payload-size>` is the number of elements in the payload vector.
    - `<payload-type-size>` is the size in bytes of each element in the payload vector.
- `beginPollProducer`
    - Emitted after polling of producer to process outbound messages has begun.
- `endPollProducer`
    - Emitted after polling of producer to process outbound messages has ended.
- `beginPollConsumer`
    - Emitted after polling of consumer to process inbound messages has begun.
- `endPollConsumer`
    - Emitted after polling of consumer to process inbound messages has ended.

## Design

This section explains the paramterization of the experiments.

The experiments are split into two rounds.

Each round compares an execution of standard Souffle (with `no-kafka` and `SNE`) against an execution of Souffle on Kafka (with either `one-kafka` or `many-kafka` and `GPCSNE`).

Each round is described below, by the value of each of the parameters that describe it.

One experiment will be generated per combination of parameters and execution mode (i.e. standard Souffle or Souffle on Kafka).

In the first round, with `no-kakfa` and `SNE` vs `many-kafka` and `GPCSNE`, we have the following.

For all in this round:

~~~
  DATASETS="soc-LiveJournal1"
  BENCHMARKS="NR"
  TYPES="number symbol"
~~~

For `no-kakfa` and `SNE`:

~~~
  THREADS="1 2 4 8 16 32 64"
  SPLITS="0"
  JOINS="none"
  SUBDIR="no-cloud"
~~~

For `many-kafka` and `GPCSNE`:

~~~
  THREADS="1"
  SPLITS="0 2 4 8 16 32 64"
  JOINS="complete left balanced"
  SUBDIR="yes-cloud"
~~~

In the second round, `no-kafka` and `SNE` vs `one-kafka` and `GPCSNE`, we have the following.

For all in this round:

~~~
  DATASETS+="cit-Patents"
  DATASETS+="com-Orkut"
  DATASETS+="com-Youtube"
  DATASETS+="roadNet-CA"
  DATASETS+="roadNet-PA"
  DATASETS+="roadNet-TX"
  DATASETS+="soc-LiveJournal1"
  DATASETS+="soc-Epinions1"
  DATASETS+="soc-Pokec"
  DATASETS+="web-BerkStan"
  DATASETS+="web-Google"
  DATASETS+="web-NotreDame"
  DATASETS+="web-Stanford"
  DATASETS+="wiki-Talk"
  DATASETS+="wiki-topcats"
  DATASETS+="prog-jenkins"
  DATASETS+="prog-jython"
  DATASETS+="prog-openjdk8"

  BENCHMARKS+="LR"
  BENCHMARKS+="RR"
  BENCHMARKS+="NR"
  BENCHMARKS+="SG"
  BENCHMARKS+="RSG"
  BENCHMARKS+="TC"
  BENCHMARKS+="SCC"
  BENCHMARKS+="MN"

  TYPES="number symbol"
  THREADS="1"
  SPLITS="0"
  JOINS="none"
  SUBDIR="no-cloud"
~~~

## Metrics

Metrics are collected from the results in the log files.
All metrics are placed in one CSV file (e.g. `metrics.csv`).
This csv file can be imported as a table into a spreadsheet.
It has all aggregation already computed, so no formula are necessary within the spreadsheet.

Each line of a metrics file is composed of the following

~~~
LINE := HEAD,BODY
HEAD := <benchmark-program>,<data-type>,<split-size>,<join-type>,<kafka-mode>,<evaluation-algorithm>,<dataset-name>,<thread-count>,<unique-id>
BODY := communicationTime,<stratum-index>,<runtime>
    | computationTime,<stratum-index>,<runtime>
    | runTime,<stratum-index>,<runtime>
    | totalCommunicationTime,<runtime>
    | totalComputationTime,<runtime>
    | totalRunTime,<runtime>
    | inputSize,<tuple-count>
    | outputSize,<tuple-count>
    | stringBytesProduced,<stratum-index>,<byte-count>
    | relationTuplesProduced,<relation-name>,<tuple-count>
    | totalStringBytesProduced,<byte-count>
    | totalRelationTuplesProduced,<tuple-count>
~~~

Metrics are to be computed for each log file generated by the experiments.

Each of the metrics to be computed is described below, with respect to the messages in the current log file.

For the `HEAD`...

- `<benchmark-program>,<data-type>,<split-size>,<join-type>,<kafka-mode>,<evaluation-algorithm>,<dataset-name>,<thread-count>,<unique-id>`
    - This is obtained from the single `printMetadata` message in the log file.

For the `BODY`...

- `communicationTime,<stratum-index>,<runtime>`
    - Per stratum sum of the `<timestamp>` fields for each `beginClient`, `endProduction`, `endConsumption`, `endProduce`, `endConsume`, `endPollProducer`, `endPollConsumer` log messages.
- `computationTime,<stratum-index>,<runtime>`
    - Per stratum value of `runTime` minus `communicationTime`.
- `runTime,<stratum-index>,<runtime>`
    - Per stratum value of the `<timestamp>` field of `endSouffleProgram`.
- `totalCommunicationTime,<runtime>`
    - Sum of `communicationTime` over all strata.
- `totalComputationTime,<runtime>`
    - Sum of `computationTime` over all strata.
- `totalRunTime,<runtime>`
    - Sum of `runTime` over all strata.
- `inputSize,<tuple-count>`
    - The `<line-count>` field of `downloadInput` log message.
- `outputSize,<tuple-count>`
    - The `<line-count>` field of `uploadOutput` log message.
- `stringBytesProduced,<stratum-index>,<byte-count>`
    - Per stratum sum of `<payload-size>` fields of all `beginProduce` messages
    having a `<topic-name>` field which is an integer or 'none'.
- `relationTermsProduced,<relation-name>,<tuple-count>`
    - Sum of `<payload-size>` fields of all `beginProduce` messages
    having a `<topic-name>` of `<relation-name>`.
- `totalStringBytesProduced,<byte-count>`
    - Sum of `stringBytesProduced` over all strata.
- `totalRelationTuplesProduced,<tuple-count>`
    - Sum of `relationsTuplesProduced` over all relations.
    
**Note: All negative values for `<stratum-index>` must be changed to 'none' before processing.**
    
### Example

As an example, consider the following log file:

~~~
main,0,15,printMetadata,NR,number,0,none,no-kafka,SNE,1,complete-graph-4,1,1603605921
main,1,301,beginBashScript
main,2,2875,downloadInput,12
main,3,36,beginSouffleProgram
main,4,67,endSouffleProgram
main,5,1223,uploadOutput,16
main,6,37,endBashScript
~~~

This is the log file generated in the `output/log` directory by an experiment.

The generation of the metrics for this log are as follows.


First, we get the `HEAD` from the log message

~~~
main,0,15,printMetadata,NR,number,0,none,no-kafka,SNE,1,complete-graph-4,1,1603605921
~~~

This gives us a `HEAD` value of `NR,number,0,none,no-kafka,SNE,1,complete-graph-4,1,1603605921`.

We then continue processing the log file.

To save space, we use `HEAD` to mean `NR,number,0,none,no-kafka,SNE,1,complete-graph-4,1,1603605921`.

The log message

~~~
main,4,67,endSouffleProgram
~~~

gives us the metric

~~~
HEAD,totalRuntime,67
~~~

The log message 

~~~
main,2,2875,downloadInput,12
~~~

gives the metric

~~~
HEAD,inputSize,12
~~~

and from 

~~~
main,5,1223,uploadOutput,16
~~~

we obtain 

~~~
HEAD,outputSize,16
~~~

The final set of metrics generated by the program are:

~~~
NR,number,0,none,no-kafka,SNE,1,complete-graph-4,1,1603605921,totalRuntime,67
NR,number,0,none,no-kafka,SNE,1,complete-graph-4,1,1603605921,inputSize,12
NR,number,0,none,no-kafka,SNE,1,complete-graph-4,1,1603605921,outputSize,16
~~~

with `HEAD` now replaced by its value.


Notice that many of the metric types (e.g. `stringBytesProduced`, `communicationTime`) do not appear here.

This is because there is no messages in the log file from which they can be computed.

In general, all metrics will be generated if an experiment uses Kafka, but only the three in this example will be generated if it does not.

A log file uses Kafka unless it contains `no-kafka` in its `printMetadata` log message.

